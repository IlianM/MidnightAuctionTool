#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de debug pour analyser la structure HTML de Leboncoin
"""

import requests
from bs4 import BeautifulSoup
import re

def analyze_leboncoin_structure():
    """Analyse la structure HTML d'une page de rÃ©sultats Leboncoin"""
    
    url = "https://www.leboncoin.fr/recherche?category=2&text=bmw%20118d&regdate=2011-2013&mileage=90000-110000"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.leboncoin.fr/'
    }
    
    print("ğŸ” ANALYSE DE LA STRUCTURE HTML LEBONCOIN")
    print("=" * 60)
    print(f"ğŸŒ URL analysÃ©e: {url}")
    
    try:
        session = requests.Session()
        session.headers.update(headers)
        response = session.get(url, timeout=15)
        
        print(f"ğŸ“Š Statut: {response.status_code}")
        print(f"ğŸ“ Taille: {len(response.content)} bytes")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Rechercher diffÃ©rents types de conteneurs
        print("\nğŸ” RECHERCHE DE CONTENEURS D'ANNONCES:")
        
        # Test 1: Liens vers annonces
        links = soup.find_all('a', href=re.compile(r'/\w+/\d+\.htm'))
        print(f"ğŸ“Œ Liens vers annonces (.htm): {len(links)}")
        
        # Test 2: Ã‰lÃ©ments avec data-qa-id
        qa_containers = soup.find_all(attrs={'data-qa-id': re.compile(r'.*ad.*', re.I)})
        print(f"ğŸ“Œ Ã‰lÃ©ments data-qa-id contenant 'ad': {len(qa_containers)}")
        
        # Test 3: Classes contenant 'ad' ou 'card'
        ad_classes = soup.find_all(class_=re.compile(r'.*(ad|card).*', re.I))
        print(f"ğŸ“Œ Classes contenant 'ad' ou 'card': {len(ad_classes)}")
        
        # Test 4: Recherche de prix (pattern â‚¬ )
        price_elements = soup.find_all(string=re.compile(r'\d+.*â‚¬'))
        print(f"ğŸ“Œ Ã‰lÃ©ments contenant des prix: {len(price_elements)}")
        
        # Analyser les premiÃ¨res annonces trouvÃ©es
        if links:
            print(f"\nğŸ“‹ ANALYSE DES PREMIERS LIENS:")
            for i, link in enumerate(links[:3]):
                print(f"\n--- Annonce {i+1} ---")
                print(f"ğŸ”— Href: {link.get('href', 'N/A')}")
                
                # Chercher le conteneur parent de ce lien
                parent = link.parent
                level = 0
                while parent and level < 5:
                    print(f"Parent niveau {level}: {parent.name} - classes: {parent.get('class', [])}")
                    
                    # Chercher le titre dans ce parent
                    title_candidates = parent.find_all(['h1', 'h2', 'h3', 'h4', 'p'], string=re.compile(r'BMW|bmw', re.I))
                    if title_candidates:
                        print(f"  ğŸ·ï¸  Titre trouvÃ©: {title_candidates[0].get_text(strip=True)}")
                    
                    # Chercher le prix dans ce parent
                    price_candidates = parent.find_all(string=re.compile(r'\d+.*â‚¬'))
                    if price_candidates:
                        print(f"  ğŸ’° Prix trouvÃ©: {price_candidates[0].strip()}")
                    
                    # Chercher l'annÃ©e dans ce parent
                    year_candidates = parent.find_all(string=re.compile(r'\b20[01][0-9]\b'))
                    if year_candidates:
                        print(f"  ğŸ“… AnnÃ©e trouvÃ©e: {year_candidates[0].strip()}")
                    
                    # Chercher le kilomÃ©trage dans ce parent
                    km_candidates = parent.find_all(string=re.compile(r'\d+.*km', re.I))
                    if km_candidates:
                        print(f"  ğŸƒ KilomÃ©trage trouvÃ©: {km_candidates[0].strip()}")
                    
                    parent = parent.parent
                    level += 1
                    
                    if level >= 2:  # Limiter pour Ã©viter trop de verbose
                        break
        
        # Sauvegarder un Ã©chantillon du HTML pour inspection manuelle
        print(f"\nğŸ’¾ SAUVEGARDE D'UN Ã‰CHANTILLON HTML...")
        with open('debug_sample.html', 'w', encoding='utf-8') as f:
            # Sauvegarder juste la partie avec les annonces
            ads_section = soup.find('div', string=re.compile(r'annonces?', re.I))
            if ads_section:
                f.write(str(ads_section.parent))
            else:
                f.write(str(soup)[:50000])  # Premier 50KB du HTML
        
        print("âœ… Ã‰chantillon sauvegardÃ© dans 'debug_sample.html'")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")

if __name__ == "__main__":
    analyze_leboncoin_structure() 