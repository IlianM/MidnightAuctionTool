#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de recherche et analyse d'annonces automobiles sur Leboncoin.fr
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
import statistics
from typing import List, Dict, Optional
from urllib.parse import urlencode, quote

class LeboncoinScraper:
    """Classe pour scraper les annonces Leboncoin"""
    
    def __init__(self):
        self.base_url = "https://www.leboncoin.fr/recherche"
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]
        
        # Configuration de session pour Ã©viter les blocages
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
    def build_search_url(self, modele: str, annee_min: int, annee_max: int, 
                        km_min: int, km_max: int) -> str:
        """Construit l'URL de recherche Leboncoin"""
        params = {
            'category': '2',  # CatÃ©gorie voitures
            'text': modele,
            'regdate': f'{annee_min}-{annee_max}',
            'mileage': f'{km_min}-{km_max}'
        }
        return f"{self.base_url}?{urlencode(params, quote_via=quote)}"
    
    def get_random_user_agent(self) -> str:
        """Retourne un User-Agent alÃ©atoire"""
        return random.choice(self.user_agents)
    
    def extract_price(self, price_text: str) -> Optional[int]:
        """Extrait le prix numÃ©rique d'un texte"""
        if not price_text:
            return None
        # Nettoyer le texte (espaces normaux, insÃ©cables, etc.)
        clean_text = price_text.replace('\u00A0', ' ').replace('&nbsp;', ' ').strip()
        # Recherche d'un nombre (avec espaces possibles) suivi de â‚¬
        price_match = re.search(r'(\d+(?:[\s\u00A0]+\d+)*)\s*â‚¬', clean_text)
        if price_match:
            # Supprimer tous les espaces et caractÃ¨res non-numÃ©riques sauf les chiffres
            price_str = re.sub(r'[^\d]', '', price_match.group(1))
            return int(price_str)
        return None
    
    def extract_year(self, text: str) -> Optional[int]:
        """Extrait l'annÃ©e d'un texte"""
        if not text:
            return None
        # Recherche d'une annÃ©e (4 chiffres entre 1990 et 2030)
        year_match = re.search(r'\b(19[9]\d|20[0-3]\d)\b', text)
        if year_match:
            return int(year_match.group(1))
        return None
    
    def extract_mileage(self, text: str) -> Optional[int]:
        """Extrait le kilomÃ©trage d'un texte"""
        if not text:
            return None
        # Recherche de kilomÃ©trage (nombre suivi de km)
        km_match = re.search(r'(\d+(?:\s?\d+)*)\s*km', text.replace(' ', ''), re.IGNORECASE)
        if km_match:
            return int(km_match.group(1).replace(' ', ''))
        return None
    
    def scrape_page(self, url: str) -> List[Dict]:
        """Scrape une page de rÃ©sultats Leboncoin"""
        # Mise Ã  jour du User-Agent pour cette requÃªte
        self.session.headers.update({
            'User-Agent': self.get_random_user_agent(),
            'Referer': 'https://www.leboncoin.fr/'
        })
        
        try:
            response = self.session.get(url, timeout=15, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            annonces = []
            
            # Recherche des annonces (sÃ©lecteurs Leboncoin actualisÃ©s)
            ads_containers = soup.find_all('article', {'data-qa-id': 'aditem_container'})
                
            for container in ads_containers:
                try:
                    # Extraction du titre
                    title_elem = container.find(attrs={'data-test-id': 'adcard-title'})
                    title = title_elem.get_text(strip=True) if title_elem else ""
                    
                    # Extraction du prix
                    price_elem = container.find(attrs={'data-test-id': 'price'})
                    price_text = price_elem.get_text(strip=True) if price_elem else ""
                    price = self.extract_price(price_text)
                    
                    # Extraction du lien
                    link_elem = container.find('a', href=re.compile(r'/ad/voitures/'))
                    link = ""
                    if link_elem and link_elem.get('href'):
                        link = link_elem['href']
                        if link.startswith('/'):
                            link = f"https://www.leboncoin.fr{link}"
                    
                    # Extraction des paramÃ¨tres (annÃ©e, kilomÃ©trage, etc.)
                    params_elem = container.find(attrs={'data-test-id': 'ad-params-light'})
                    params_text = params_elem.get_text(strip=True) if params_elem else ""
                    
                    # Alternative: recherche dans les labels dÃ©taillÃ©s
                    if not params_text:
                        labels_container = container.find(attrs={'data-test-id': 'ad-params-labels'})
                        if labels_container:
                            params_text = labels_container.get_text(strip=True)
                    
                    # Extraction de l'annÃ©e et du kilomÃ©trage
                    combined_text = f"{title} {params_text}"
                    year = self.extract_year(combined_text)
                    mileage = self.extract_mileage(combined_text)
                    
                    # Ajouter l'annonce si elle a des donnÃ©es valides ET si elle correspond au modÃ¨le recherchÃ©
                    if title and price and year and mileage:
                        annonces.append({
                            'titre': title,
                            'prix': price,
                            'annee': year,
                            'kilometrage': mileage,
                            'lien': link
                        })
                        
                except Exception as e:
                    # Ignorer silencieusement les erreurs d'extraction individuelles
                    continue
            
            return annonces
            
        except requests.RequestException as e:
            print(f"âŒ Erreur lors de la requÃªte: {e}")
            if "403" in str(e) or "Forbidden" in str(e):
                print("ğŸš« Erreur 403 - Leboncoin bloque probablement les requÃªtes automatisÃ©es")
                print("ğŸ’¡ Suggestions:")
                print("   - VÃ©rifiez que l'URL fonctionne dans un navigateur")
                print("   - Essayez avec des critÃ¨res de recherche diffÃ©rents")
                print("   - Attendez quelques minutes avant de relancer")
            return []
    
    def is_relevant_ad(self, title: str, modele: str) -> bool:
        """VÃ©rifie si une annonce correspond au modÃ¨le recherchÃ©"""
        title_lower = title.lower()
        modele_parts = modele.lower().split()
        
        # VÃ©rifier que tous les mots-clÃ©s du modÃ¨le sont prÃ©sents
        for part in modele_parts:
            if part not in title_lower:
                return False
        return True
    
    def search_ads(self, modele: str, annee_min: int, annee_max: int, 
                   km_min: int, km_max: int, nb_annonces: int = 50) -> List[Dict]:
        """Recherche des annonces selon les critÃ¨res spÃ©cifiÃ©s"""
        
        url = self.build_search_url(modele, annee_min, annee_max, km_min, km_max)
        print(f"\nğŸ”— Lien de recherche: {url}")
        print(f"ğŸ” Recherche: {modele} | {annee_min}â€“{annee_max} | {km_min:,}â€“{km_max:,} km")
        print("â³ RÃ©cupÃ©ration en cours...")
        
        # Attendre un peu avant la premiÃ¨re requÃªte
        time.sleep(random.uniform(1, 2))
        
        all_ads = []
        page = 1
        max_pages = 10  # Limite pour Ã©viter les boucles infinies
        
        while len(all_ads) < nb_annonces and page <= max_pages:
            # Construction de l'URL avec pagination
            page_url = f"{url}&page={page}" if page > 1 else url
            
            # Scraping de la page
            raw_ads = self.scrape_page(page_url)
            
            if not raw_ads:
                break
            
            # Filtrer les annonces pertinentes
            filtered_ads = [ad for ad in raw_ads if self.is_relevant_ad(ad['titre'], modele)]
            
            all_ads.extend(filtered_ads)
            
            if filtered_ads:
                print(f"ğŸ“„ Page {page}: {len(filtered_ads)} annonces pertinentes rÃ©cupÃ©rÃ©es (total: {len(all_ads)})")
            else:
                print(f"ğŸ“„ Page {page}: aucune annonce pertinente trouvÃ©e")
            
            # DÃ©lai entre les requÃªtes (plus long pour Ã©viter les blocages)
            if page < max_pages and len(all_ads) < nb_annonces:
                time.sleep(random.uniform(3, 6))
            page += 1
        
        # Limiter au nombre demandÃ©
        return all_ads[:nb_annonces]

class DataAnalyzer:
    """Classe pour analyser les donnÃ©es des annonces"""
    
    @staticmethod
    def analyze_prices(annonces: List[Dict]) -> Dict:
        """Analyse statistique des prix"""
        if not annonces:
            return {}
        
        prices = [ad['prix'] for ad in annonces if ad.get('prix')]
        
        if not prices:
            return {}
        
        return {
            'minimum': min(prices),
            'maximum': max(prices),
            'moyenne': int(statistics.mean(prices)),
            'mediane': int(statistics.median(prices)),
            'nombre': len(prices)
        }
    
    @staticmethod
    def display_results(annonces: List[Dict], criteres: Dict):
        """Affiche les rÃ©sultats de l'analyse"""
        print("\n" + "="*60)
        print("ğŸ“Š RÃ‰SULTATS DE L'ANALYSE")
        print("="*60)
        
        # Affichage des critÃ¨res de recherche
        print(f"ğŸ” Recherche : {criteres['modele']} ({criteres['annee_min']}â€“{criteres['annee_max']}), "
              f"{criteres['km_min']:,}â€“{criteres['km_max']:,} km")
        print(f"ğŸ“„ Annonces analysÃ©es : {len(annonces)}")
        
        if len(annonces) < 10:
            print("âš ï¸  AVERTISSEMENT: Moins de 10 annonces trouvÃ©es. "
                  "Les statistiques peuvent ne pas Ãªtre reprÃ©sentatives.")
        
        # Analyse des prix
        stats = DataAnalyzer.analyze_prices(annonces)
        
        if stats:
            print(f"\nğŸ’° Prix minimum : {stats['minimum']:,} â‚¬")
            print(f"ğŸ’° Prix maximum : {stats['maximum']:,} â‚¬")
            print(f"ğŸ“ˆ Prix moyen : {stats['moyenne']:,} â‚¬")
            print(f"ğŸ“Š Prix mÃ©dian : {stats['mediane']:,} â‚¬")
        else:
            print("\nâŒ Impossible de calculer les statistiques de prix")
        
        # Affichage des liens
        if annonces:
            print(f"\nğŸ”— Liens des annonces:")
            for i, ad in enumerate(annonces, 1):
                if ad.get('lien'):
                    print(f"- {ad['lien']}")
                else:
                    print(f"- Annonce {i}: {ad.get('titre', 'Titre non disponible')}")

def get_user_input():
    """RÃ©cupÃ¨re les paramÃ¨tres de recherche depuis la console"""
    print("ğŸš— RECHERCHE LEBONCOIN - VOITURES")
    print("="*40)
    
    modele = input("ğŸ·ï¸  Nom du modÃ¨le (ex: BMW 118d): ").strip()
    
    while True:
        try:
            annee_min = int(input("ğŸ“… AnnÃ©e minimum (ex: 2011): "))
            break
        except ValueError:
            print("âŒ Veuillez entrer une annÃ©e valide")
    
    while True:
        try:
            annee_max = int(input("ğŸ“… AnnÃ©e maximum (ex: 2013): "))
            if annee_max >= annee_min:
                break
            else:
                print("âŒ L'annÃ©e maximum doit Ãªtre supÃ©rieure ou Ã©gale Ã  l'annÃ©e minimum")
        except ValueError:
            print("âŒ Veuillez entrer une annÃ©e valide")
    
    while True:
        try:
            km_min = int(input("ğŸƒ KilomÃ©trage minimum (ex: 90000): "))
            break
        except ValueError:
            print("âŒ Veuillez entrer un kilomÃ©trage valide")
    
    while True:
        try:
            km_max = int(input("ğŸƒ KilomÃ©trage maximum (ex: 110000): "))
            if km_max >= km_min:
                break
            else:
                print("âŒ Le kilomÃ©trage maximum doit Ãªtre supÃ©rieur ou Ã©gal au minimum")
        except ValueError:
            print("âŒ Veuillez entrer un kilomÃ©trage valide")
    
    nb_annonces_input = input("ğŸ“Š Nombre d'annonces Ã  analyser (dÃ©faut: 50): ").strip()
    nb_annonces = 50
    if nb_annonces_input:
        try:
            nb_annonces = int(nb_annonces_input)
        except ValueError:
            print("âŒ Nombre invalide, utilisation de la valeur par dÃ©faut: 50")
    
    return {
        'modele': modele,
        'annee_min': annee_min,
        'annee_max': annee_max,
        'km_min': km_min,
        'km_max': km_max,
        'nb_annonces': nb_annonces
    }

def main():
    """Fonction principale"""
    try:
        # RÃ©cupÃ©ration des paramÃ¨tres utilisateur
        criteres = get_user_input()
        
        # Initialisation du scraper
        scraper = LeboncoinScraper()
        
        # Recherche des annonces
        annonces = scraper.search_ads(
            criteres['modele'],
            criteres['annee_min'],
            criteres['annee_max'],
            criteres['km_min'],
            criteres['km_max'],
            criteres['nb_annonces']
        )
        
        # Affichage des rÃ©sultats
        DataAnalyzer.display_results(annonces, criteres)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Recherche interrompue par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur inattendue: {e}")

if __name__ == "__main__":
    main()